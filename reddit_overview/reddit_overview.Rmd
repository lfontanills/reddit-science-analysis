---
title: "NLP with R: Analyzing top posts on r/science"
author: "Laura Fontanills"
date: "`r format(Sys.time(), '%d %B %Y')`"
mail: "lfontanills@gmail.com"
linkedin: "lfontanills"
github: "lfontanills"
home: "lfontanills.github.io"
# !!! You need to provide a logo image here !!! Or just delete the field for no logo
# logo: "logo_gallery.png"
output:
  epuRate::epurate:
    toc: TRUE
    number_sections: FALSE
    code_folding: "hide"
---


<br><br>

<!-- > Let's start with a short introduction. Explain briefly what this document is going to talk about. May be add a [useful link](https://github.com/holtzy/epuRate) relative to this project. -->

> I've read reddit.com/r/science for years, and I had casually observed some patterns in the types of posts that were most upvoted. I wanted to test whether my instincts were correct about the types of posts users favored. 

> In December 2022 I used a python script to scrape the top posts from reddit.com/r/science. I scraped the top 100 posts of all time, the top 100 posts of the past year, and the top 100 posts of the past month.  I'm hoping that looking at three time periods (past-month, past-year, and all-time) helps me see if any topics are amplified or diminished by selection (upvoting) over time. The subreddit has grown over time, so more recent posts are favored in the all-time list.

> Posts are ranked based on their "score" (or, number of upvotes).  The subreddit can receive dozens of posts in a single day, so I believe that my sample is an accurate representation of the most popular topics. 

# Scraping Reddit with PRAW

Here's the python script I used to scrape Reddit. I removed my personal information, so the script won't run here; to run this code yourself, you'll need to follow the instructions [this guide](https://praw.readthedocs.io/en/stable/) and use your own credentials.

```{python eval=FALSE, include=FALSE}
# import packages
import praw
import pandas

# read-only instance
reddit_read_only = praw.Reddit(
    client_id="", #your info here
    client_secret="", #your info here
    user_agent="", #your info here
)

# extract subreddit information

subreddit = reddit_read_only.subreddit("science")

# display subreddit name

print("Display Name:", subreddit.display_name)

# display subreddit title
print("Title:", subreddit.title)

# display subreddit description
print("Description:", subreddit.description)

# get top posts this from time period
# all = all time
# year = past year
# month = past month
posts = subreddit.top("all")

posts_dict = { 
    "id": [],
    "created_unix_utc": [],
    "post_url": [],
    "post_title": [],
    "flair": [],
    "score": [],
    "num_comments": [],
    "upvote_ratio": []
}

for post in posts:
    posts_dict["id"].append(post.id)
    posts_dict["created_unix_utc"].append(post.created_utc)
    posts_dict["post_url"].append(post.url)
    posts_dict["post_title"].append(post.title)
    posts_dict["flair"].append(post.link_flair_text)
    posts_dict["score"].append(post.score)
    posts_dict["num_comments"].append(post.num_comments)
    posts_dict["upvote_ratio"].append(post.upvote_ratio)

# change this when scraping different time periods
top_posts_all= pandas.DataFrame(posts_dict)
top_posts_all

top_posts_all.to_csv("Top-Posts-All.csv")

```


# Get the data
***
```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(skimr)
library(textdata) # sentiment analysis
library(readr) # NLP
library(gghighlight) # graphing
library(wordcloud) 
library(rmarkdown)    # You need this library to run this template.
library(epuRate)      # Install with devtools: install_github("holtzy/epuRate", force=TRUE)
options(dplyr.summarise.inform = FALSE)
```

I created three data frames, one for the top posts of all-time, one for the top posts last year, and one for the top posts last month (as of December 2022).
```{r}
# Whole dataset
top_all <- read.csv("~/Documents/Projects/reddit-science-analysis-2/top_all.csv")
top_year <- read.csv("~/Documents/Projects/reddit-science-analysis-2/top_year.csv")
top_month <- read.csv("~/Documents/Projects/reddit-science-analysis-2/top_month.csv")

# Post titles only
tidy_all <- read.csv("~/Documents/Projects/reddit-science-analysis-2/tidy_all.csv")
tidy_year <- read.csv("~/Documents/Projects/reddit-science-analysis-2/tidy_year.csv")
tidy_month <- read.csv("~/Documents/Projects/reddit-science-analysis-2/tidy_month.csv")

```

The top_all file has `r nrow(top_all)` lines and `r ncol(top_all)` columns. It is ready to be analyzed.

The top_year file has `r nrow(top_year)` lines and `r ncol(top_year)` columns. It is ready to be analyzed.

The top_month file has `r nrow(top_month)` lines and `r ncol(top_month)` columns. It is ready to be analyzed.

The tidy_all file contains `r nrow(tidy_all)` words. The tidy_year file contanils `r nrow(tidy_year` words. The tidy_month file contanils `r nrow(tidy_month)` words.

# Analysis
***
Posts are ranked by their score (which is the same as the number of upvotes). Each post has "flair" which designates the topic. Each post has exactly 1 topic flair. I chose to use assigned flair for this analysis, rather than designating my own topics.

A quick look at the numerical values in my data sets showed that score, number of comments, and upvote ratio (the ratio of upvote to downvotes) scaled with post rank (see appendix). I decided to focus instead on the number of posts by topic. 

It is worth noting that there are a few posts with very low upvote ratios (less than 75%), even though they also have very high scores. This indicates high engagement of users with both positive and negative opinions about the post. Effectively all of these posts are about contentious or divisive issues, and many are US-centric. Broadly, these posts are about:
* Top_month: Marijuana, women's sexual desire
* Top_year: US politics, US gun violence, LGBTQ+, covid-19 vaccines, Gen-Z climate change beliefs, Bitcoin,
racism, Trump, sexism, refugees in the US, neoliberalism, abortion, religion
* Top_all: US comedy-news programs, Trump

```{r}
# find post with very low upvote ratio
# all time
top_all %>% 
  filter(upvote_ratio < .75 ) %>% 
  arrange(upvote_ratio) %>% 
  select(c(post_title, flair, upvote_ratio, score, domain))

# past year
top_year %>% 
  filter(upvote_ratio < .75 ) %>% 
  arrange(upvote_ratio) %>% 
  select(c(post_title, flair, upvote_ratio, score, domain))

# past month
top_month %>% 
  filter(upvote_ratio < .75 ) %>% 
  arrange(upvote_ratio) %>% 
  select(c(post_title, flair, upvote_ratio, score, domain))

```

## Finding 1: Relationship between topic and post score

Three topics dominate the top-post lists for all timeframes: Psychology, Health, and Social Science. Together, these make up 51% of the top posts of all time (59% top month, 56% top year). 

```{r}
# all-time
by_flair_all <- top_all %>% 
  group_by(flair) %>% 
  summarize(count_id=n_distinct(id)) %>% 
  arrange(desc(count_id)) %>% 
  ggplot(aes(x = count_id, y=reorder(flair, count_id), fill=flair)) +
  geom_col(show.legend=FALSE) +
  gghighlight(count_id > 10) +
  labs(
    title = "Top r/science posts by topic",
    subtitle = "Top 100 posts of all time",
    x = "Number of Posts",
    y = "Topic"
  ) +
  theme_minimal()
by_flair_all

# past year
by_flair_year <- top_year %>% 
  group_by(flair) %>% 
  summarize(count_id=n_distinct(id)) %>% 
  arrange(desc(count_id)) %>% 
  ggplot(aes(x = count_id, y=reorder(flair, count_id), fill=flair)) +
  geom_col(show.legend=FALSE) +
  gghighlight(count_id > 10) +
  labs(
    title = "Top r/science posts by topic",
    subtitle = "Top 100 posts last year (2022)",
    x = "Number of Posts",
    y = "Topic"
  ) +
  theme_minimal()
by_flair_year

# past month
by_flair_month <- top_month %>% 
  group_by(flair) %>% 
  summarize(count_id=n_distinct(id)) %>% 
  arrange(desc(count_id)) %>% 
  ggplot(aes(x = count_id, y=reorder(flair, count_id), fill=flair)) +
  geom_col(show.legend=FALSE) +
  gghighlight(count_id > 10) +
  labs(
    title = "Top r/science posts by topic",
    subtitle = "Top 100 posts last month (December 2022)",
    x = "Number of Posts",
    y = "Topic"
  ) +
  theme_minimal()
by_flair_month
```

## Finding 2: Relationship between sources and post score

The top posts link to 60+ source websites (identified by domain name, e.g. academictimes.com). For the top posts of all time, no single source has more than 5 posts linked to it. However, for the past year, 14 unique posts come from psypost.org. For the past month, 17 posts come from psypost.org. This indicates a recent increase in the popularity of posts from this website.

```{r}
# all-time
by_domain_all <- top_all %>% 
  group_by(domain) %>% 
  summarize(count_id=n_distinct(id)) %>% 
  arrange(desc(count_id)) %>% 
  head(10) %>% 
  ggplot(aes(x = count_id, y=reorder(domain, count_id), fill=domain)) +
  geom_col(show.legend=FALSE) +
  gghighlight(count_id > 10) +
  labs(
    title = "Top 10 sources for r/science",
    subtitle = "For the top 100 posts of all time",
    x = "Number of Posts",
    y = "Topic"
  ) +
  theme_minimal()
by_domain_all

# past year
by_domain_year <- top_year %>% 
  group_by(domain) %>% 
  summarize(count_id=n_distinct(id)) %>% 
  arrange(desc(count_id)) %>% 
  head(10) %>% 
  ggplot(aes(x = count_id, y=reorder(domain, count_id), fill=domain)) +
  geom_col(show.legend=FALSE) +
  gghighlight(count_id > 10) +
  labs(
    title = "Top 10 sources for r/science",
    subtitle = "Top 100 posts last year (2022)",
    x = "Number of Posts",
    y = "Topic"
  ) +
  theme_minimal()
by_domain_year

# past month
by_domain_month <- top_month %>% 
  group_by(domain) %>% 
  summarize(count_id=n_distinct(id)) %>% 
  arrange(desc(count_id)) %>% 
  head(10) %>% 
  ggplot(aes(x = count_id, y=reorder(domain, count_id), fill=domain)) +
  geom_col(show.legend=FALSE) +
  gghighlight(count_id > 10) +
  labs(
    title = "Top 10 sources for r/science",
    subtitle = "Top 100 posts last month (December 2022)",
    x = "Number of Posts",
    y = "Topic"
  ) +
  theme_minimal()
by_domain_month
```

## Finding 3: Word frequency in post titles

To find the most common words in top post titles, I created dataframes with just post titles, then used unnest_tokens() to separate each word into its own column. I filtered out stopwords and created wordclouds to show the 30 most frequent words for the last month, year, and all time. 
```{r}

tidy_all %>% 
  count(word) %>% 
  with(wordcloud(word, n, colors=colorRampPalette(brewer.pal(9,"BuPu"))(50), max.words = 20))

tidy_year %>% 
  count(word) %>% 
  with(wordcloud(word, n, colors=colorRampPalette(brewer.pal(9,"BuPu"))(50), max.words = 20))

tidy_month %>% 
  count(word) %>% 
  with(wordcloud(word, n, colors=colorRampPalette(brewer.pal(9,"BuPu"))(50), max.words = 20))
```

## Finding 4: Comparing word frequency to other science sources


## Conclusion
***
<!-- Read more about this template [here](https://github.com/holtzy/epuRate). -->

<!-- Learn how to custom your R Markdown document [here](https://holtzy.github.io/Pimp-my-rmd/). -->
