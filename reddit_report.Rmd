---
title: "Analyzing post topics and titles on reddit.com/r/science"
author: "Laura Fontanills"
date: "`r format(Sys.time(), '%d %B %Y')`"
mail: "lfontanills@gmail.com"
linkedin: "lfontanills"
github: "lfontanills"
home: "lfontanills.github.io"
# !!! You need to provide a logo image here !!! Or just delete the field for no logo
# logo: "logo_gallery.png"
logo: "reddit_logo.png"
output:
  epuRate::epurate:
    toc: TRUE
    number_sections: FALSE
    code_folding: "hide"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages, include=FALSE}
library(tidyverse) # data processing and analysis
library(lubridate) # wrangle dates
library(skimr) # skim data frames
library(urltools) # wrangle urls
library(tidytext) # NLP toolkit
library(textdata) # Sentiment analysis
library(wordcloud2) # Wordclouds
library(gghighlight) # Adding to graphs
```

# Read this first

If you'd like to skip all the data cleaning steps, click on "Get the data" in the sidebar.

# About

I spend a lot of time on reddit, and a few months ago I noticed that one subreddit I frequent, [r/science](reddit.com/r/science), had more and more posts about pop-psychology getting tons of upvotes. Reddit is a social media site, and scientific journals are notoriously hard to access, so I expect a gap between the types of science content between them. Still, I decided to test my gut feeling that the subreddit's content was changing.

I want to see what kinds of posts made it to the top of r/science: the posts that have the most upvotes, and therefore the posts that the most people liked and engaged with. What topics interest the average r/science user the most? 

# Scraping Reddit with PRAW

I used a python script to scrape r/science top 100 posts over three time periods: the past month, the past year, and all-time. Within these time periods I obtained data for 9 different variables:
* `...1` ranks the post from 0-99 (0 being the highest rank).
* `id` is a unique string of letters and numbers that identify each post.
* `created_unix_utc` is the time the post was created (as a unix number).
* `post_url` is the url for the website the post links to. This can be from an online news outlet, journal, etc.. Each post has a unique url.
* `post_title` is the title of the post.
* `flair` is a tag attached to each post by a moderator that places that post in a category. It represents the post's topic.
* `score` is the number of times a post was upvoted (each user can upvote a post once).
* `num_comments` is the number of unique comments on a post.
* `upvote_ratio` is the ratio of upvotes (indivading user approval) to downvotes (indicating user disapproval).

The script won't run here; to run this code yourself, you'll need to follow the instructions [this guide](https://praw.readthedocs.io/en/stable/) and use your own credentials.

```{python scraper.py, eval=FALSE, include=FALSE}
# import packages
import praw
import pandas

# read-only instance
reddit_read_only = praw.Reddit(
    client_id="", #your info here
    client_secret="", #your info here
    user_agent="", #your info here
)

# extract subreddit information

subreddit = reddit_read_only.subreddit("science")

# display subreddit name

print("Display Name:", subreddit.display_name)

# display subreddit title
print("Title:", subreddit.title)

# display subreddit description
print("Description:", subreddit.description)

# get top posts this from time period
# all = all time
# year = past year
# month = past month
posts = subreddit.top("all")

posts_dict = { 
    "id": [],
    "created_unix_utc": [],
    "post_url": [],
    "post_title": [],
    "flair": [],
    "score": [],
    "num_comments": [],
    "upvote_ratio": []
}

for post in posts:
    posts_dict["id"].append(post.id)
    posts_dict["created_unix_utc"].append(post.created_utc)
    posts_dict["post_url"].append(post.url)
    posts_dict["post_title"].append(post.title)
    posts_dict["flair"].append(post.link_flair_text)
    posts_dict["score"].append(post.score)
    posts_dict["num_comments"].append(post.num_comments)
    posts_dict["upvote_ratio"].append(post.upvote_ratio)

# change this when scraping different time periods
top_posts_all= pandas.DataFrame(posts_dict)
top_posts_all

top_posts_all.to_csv("Top-Posts-All.csv")
# repeat for past year, past month
```


# Data cleaning

```{r read raw data frames, show_col_types = FALSE}
# create data frame with top 100 posts from the last month
top_month <- read_csv("Top-Posts-Month.csv")

# create data frame with top 100 posts from the last year
top_year <- read_csv("Top-Posts-Year.csv")

# create data frame with top 100 posts from all time
top_all <- read_csv("Top-Posts-All.csv")
```

I confirm that all posts are unique by checking the number of distinct post IDs. TRUE indicates that all rows are unique.

```{r check all posts unique}

n_distinct(top_all$id) == nrow(top_all)
n_distinct(top_year$id) == nrow(top_year)
n_distinct(top_month$id) == nrow(top_month)
```

I change the name of the first column to represent post rank, and add 1 to all rankings so that posts are ranked from 1 to 100.

```{r edit column names - rank}

# change column 1 name from ...1 to rank
colnames(top_all)[1] <- "all_rank"
colnames(top_year)[1] <- "year_rank"
colnames(top_month)[1] <- "month_rank"

# add 1 to all the rankings for clarity
top_all$all_rank <- top_all$all_rank + 1
top_year$year_rank <- top_year$year_rank + 1
top_month$month_rank <- top_month$month_rank + 1
```

I convert the created_unix field from a number to a datetime, and save this as `created_utc`. This shows the time each post was made in UTC.

```{r create unix_utc datetime}

top_all$created_utc <- as_datetime(top_all$created_unix_utc)
top_year$created_utc <- as_datetime(top_year$created_unix_utc)
top_month$created_utc <- as_datetime(top_month$created_unix_utc)
```

I isolate the domain name from each post url, and save this as post_url. This shows the source website of each post -- sources include news outlets, science magazines, journals, blogs, etc..

```{r create post_url chr}

top_all$domain <- domain(top_all$post_url)
top_year$domain <- domain(top_year$post_url)
top_month$domain <- domain(top_month$post_url)
```

Let's save the cleaned and transformed data to fresh data frames.

```{r create cleaned df}
all_clean <- top_all
year_clean <- top_year
month_clean <- top_month
```

I'll also make data frames containing just the post titles.

```{r create title-only df}
text_month <- all_clean[c("post_title")]
text_year <- year_clean[c("post_title")]
text_all <- month_clean[c("post_title")]
```

## Text cleaning

Later on, I will analyze word frequencies in post titles. To do this I need to restructure the data frame so that each word is in its own column (each row still represents one post).

```{r unnest tokens titles}

# restructure one token per row: unnest tokens
text_month <-text_month %>% 
  unnest_tokens(word, post_title)

text_year <- text_year %>% 
  unnest_tokens(word, post_title)

text_all <- text_all %>% 
  unnest_tokens(word, post_title)
```

I remove stopwords using the tidytext package's stopword list. Stopwords are common words like "a" and "the" that are very common and not meaningful. 

```{r stop words}

# get stop words list
data("stop_words")

text_all <- text_month %>% 
  anti_join(stop_words)

text_year<- text_year %>% 
  anti_join(stop_words)

text_month <- text_month %>% 
  anti_join(stop_words)

# check for common words

text_all %>% 
  count(word, sort = TRUE) %>% 
  head(20)

text_year %>% 
  count(word, sort = TRUE) %>% 
  head(20)

text_month %>% 
  count(word, sort = TRUE) %>% 
  head(20)

```

There are a few words in the text data frames that are generic science words, but are not in the stopword list. These include "study", "found", "suggests", and "research". There are also numbers that without context are not meaningful. These words and numbers will be filtered out.

```{r custom stop words}

# make custom stopword lists

stop_nums<- as.data.frame(as.character(1:10000))
colnames(stop_nums)[1] <- "word"

stop_science <- c("study", "found", "scientist", "scientists", "research", "researchers", "suggests", "finding")
stop_science <- as.data.frame(stop_science)
colnames(stop_science)[1] <- "word"

# remove custom stopwords

text_all_clean <- text_all %>% 
  anti_join(stop_nums) %>% 
  anti_join(stop_science)

text_year_clean <- text_year %>% 
  anti_join(stop_nums) %>% 
  anti_join(stop_science)

text_month_clean <- text_month %>% 
  anti_join(stop_nums) %>% 
  anti_join(stop_science)


# find most common words

text_all_clean %>% 
  count(word, sort = TRUE) %>% 
  head(20)

text_year_clean %>% 
  count(word, sort = TRUE) %>% 
  head(20)

text_month_clean %>% 
  count(word, sort = TRUE) %>% 
  head(20)

```

all_clean has `r nrow(all_clean)` lines and `r ncol(all_clean)` columns. It is ready to be analyzed.

year_clean has `r nrow(year_clean)` lines and `r ncol(year_clean)` columns. It is ready to be analyzed.

month_clean has `r nrow(month_clean)` lines and `r ncol(month_clean)` columns. It is ready to be analyzed.

text_all_clean has `r nrow(text_all_clean)` lines and `r ncol(text_all_clean)` columns. It is ready to be analyzed.

text_year_clean has `r nrow(text_year_clean)` lines and `r ncol(text_year_clean)` columns. It is ready to be analyzed.

text_month_clean has `r nrow(text_month_clean)` lines and `r ncol(text_month_clean)` columns. It is ready to be analyzed.

# Get the data

I created individual data frames for the top 100 posts of all time, the last year, and the last month.

I also created data frames containing just the post titles for each time period.

I did not combine these data frames because they each represent a different timescale.
```{r}
# Whole dataset
all_clean <- read.csv("~/Documents/Projects/reddit-science/all_clean.csv")
year_clean <- read.csv("~/Documents/Projects/reddit-science/year_clean.csv")
month_clean <- read.csv("~/Documents/Projects/reddit-science/month_clean.csv")

# Post titles only
text_all_clean <- read.csv("~/Documents/Projects/reddit-science/text_all_clean.csv")
text_year_clean <- read.csv("~/Documents/Projects/reddit-science/text_year_clean.csv")
text_month_clean <- read.csv("~/Documents/Projects/reddit-science/text_month_clean.csv")

```

The top_all file has `r nrow(top_all)` lines and `r ncol(top_all)` columns. It is ready to be analyzed.

The top_year file has `r nrow(top_year)` lines and `r ncol(top_year)` columns. It is ready to be analyzed.

The top_month file has `r nrow(top_month)` lines and `r ncol(top_month)` columns. It is ready to be analyzed.

The tidy_all file contains `r nrow(tidy_all)` words. The tidy_year file contanils `r nrow(tidy_year)` words. The tidy_month file contanils `r nrow(tidy_month)` words.

# Analysis and Visualizations

## Exploring the data set

A look at the data summary for top_all shows that the oldest post is from 2015, and the median post is from 2020. The subreddit has been growing over time.

```{r get summary}
summary(top_all)
summary(top_year)
summary(top_month)
```

I confirmed that post rank roughly scaled with post score -- the higher a post is ranked, the more people upvoted it, representing engagement with and interest in the post.

```{r post rank by score}

all_clean %>% 
  ggplot(aes(x=all_rank, y = score)) +
  geom_point() +
  geom_smooth() +
    labs(
    title = "Relationship between post rank and score",
    subtitle = "Top 100 posts of all time",
    x = "Rank",
    y = "Score"
  ) +
  theme_minimal()

year_clean %>% 
  ggplot(aes(x=year_rank, y = score)) +
  geom_point() +
  geom_smooth() +
    labs(
    title = "Relationship between post rank and score",
    subtitle = "Top 100 posts last year (2022)",
    x = "Rank",
    y = "Score"
  ) +
  theme_minimal()

month_clean %>% 
  ggplot(aes(x=month_rank, y = score)) +
  geom_point() +
  geom_smooth() +
    labs(
    title = "Relationship between post rank and score",
    subtitle = "Top 100 posts last month (December 2022)",
    x = "Rank",
    y = "Score"
  ) +
  theme_minimal()

```

I investigated whether post rank scaled with upvote ratios. There are a few posts with very low upvote ratios (less than 75%), even though they also have very high scores. This indicates high engagement of users with both positive and negative opinions about the post. Effectively all of these posts are about contentious or divisive issues, and many are US-centric. Based on this finding I decided to focus my analysis on post topics and sources.

```{r}
# find post with very low upvote ratio
# all time
top_all %>% 
  filter(upvote_ratio < .75 ) %>% 
  arrange(upvote_ratio) %>% 
  select(c(post_title, flair, upvote_ratio, score, domain))

# past year
top_year %>% 
  filter(upvote_ratio < .75 ) %>% 
  arrange(upvote_ratio) %>% 
  select(c(post_title, flair, upvote_ratio, score, domain))

# past month
top_month %>% 
  filter(upvote_ratio < .75 ) %>% 
  arrange(upvote_ratio) %>% 
  select(c(post_title, flair, upvote_ratio, score, domain))

```


## Finding 1: Relationship between topic and post score

Three topics dominate the top-post lists: Psychology, Health, and Social Science. Together, these make up 51% of the top posts of all time (59% top month, 56% top year). 

```{r}
# all-time
by_flair_all <- top_all %>% 
  group_by(flair) %>% 
  summarize(count_id=n_distinct(id)) %>% 
  arrange(desc(count_id)) %>% 
  ggplot(aes(x = count_id, y=reorder(flair, count_id), fill=flair)) +
  geom_col(show.legend=FALSE) +
  gghighlight(count_id > 10) +
  labs(
    title = "Top r/science posts by topic",
    subtitle = "Top 100 posts of all time",
    x = "Number of Posts",
    y = "Topic"
  ) +
  theme_minimal()
by_flair_all

# past year
by_flair_year <- top_year %>% 
  group_by(flair) %>% 
  summarize(count_id=n_distinct(id)) %>% 
  arrange(desc(count_id)) %>% 
  ggplot(aes(x = count_id, y=reorder(flair, count_id), fill=flair)) +
  geom_col(show.legend=FALSE) +
  scale_fill_brewer(palette = "BuGn") +
  gghighlight(count_id > 10) +
  labs(
    title = "Top r/science posts by topic",
    subtitle = "Top 100 posts last year (2022)",
    x = "Number of Posts",
    y = "Topic"
  ) +
  theme_minimal()
by_flair_year

# past month
by_flair_month <- top_month %>% 
  group_by(flair) %>% 
  summarize(count_id=n_distinct(id)) %>% 
  arrange(desc(count_id)) %>% 
  ggplot(aes(x = count_id, y=reorder(flair, count_id), fill=flair)) +
  geom_col(show.legend=FALSE) +
  gghighlight(count_id > 10) +
  scale_fill_brewer(palette = "BuPu", direction = -1) +
  labs(
    title = "Top r/science posts by topic",
    subtitle = "Top 100 posts last month (December 2022)",
    x = "Number of Posts",
    y = "Topic"
  ) +
  theme_minimal()
by_flair_month
```

## Finding 2: Relationship between sources and post score

The posts come from 60+ source websites (identified by domain name, e.g. academictimes.com). For the all-time list, no source has more than 5 posts associated with it. However, in the past-year list, 14 unique posts come from psypost.org. In the past-month list, 17 posts come from psypost.org. This indicates a recent increase in the popularity of posts from this psypost.org in particular.

```{r}
# all-time
by_domain_all <- top_all %>% 
  group_by(domain) %>% 
  summarize(count_id=n_distinct(id)) %>% 
  arrange(desc(count_id)) %>% 
  head(10) %>% 
  ggplot(aes(x = count_id, y=reorder(domain, count_id), fill=domain)) +
  geom_col(show.legend=FALSE) +
  gghighlight(count_id > 10) +
  labs(
    title = "Most common sources for r/science posts",
    subtitle = "Top 100 posts of all time",
    x = "Number of Posts",
    y = "Topic"
  ) +
  theme_minimal()
by_domain_all

# past year
by_domain_year <- top_year %>% 
  group_by(domain) %>% 
  summarize(count_id=n_distinct(id)) %>% 
  arrange(desc(count_id)) %>% 
  head(10) %>% 
  ggplot(aes(x = count_id, y=reorder(domain, count_id), fill=domain)) +
  geom_col(show.legend=FALSE) +
  scale_fill_brewer(palette = "Greens", direction = -1) +
  gghighlight(count_id > 10 ) +
  labs(
    title = "Most common sources for r/science posts",
    subtitle = "Top 100 posts last year (2022)",
    x = "Number of Posts",
    y = "Topic"
  ) +
  theme_minimal()
by_domain_year

# past month
by_domain_month <- top_month %>% 
  group_by(domain) %>% 
  summarize(count_id=n_distinct(id)) %>% 
  arrange(desc(count_id)) %>% 
  head(10) %>% 
  ggplot(aes(x = count_id, y=reorder(domain, count_id), fill=domain)) +
  geom_col(show.legend=FALSE) +
  gghighlight(count_id > 10) +
  scale_fill_brewer(palette = "PRGn") +
  labs(
    title = "Most common sources for r/science posts",
    subtitle = "Top 100 posts last month (December 2022)",
    x = "Number of Posts",
    y = "Topic"
  ) +
  theme_minimal()
by_domain_month
```

## Finding 3: Word frequency in post titles

I created wordclouds to show the most frequent words in the titles of the top posts of all time. The word "people" appeared 12 times, "children" appeared 8 times, "sex" appeared 8 times, "life" appeared 7 times, and "U.S." appeared 6 times.

```{r}

all_words <- text_all_clean %>% count(word, sort=TRUE)
wordcloud2(all_words, size = 1.6)
```



The past-year word list represents more recent trends on the subreddit. The word "black" appeared 8 times, "woman" appeared 8 times, and "lung" appeared 7 times.
```{r}

year_words <- text_year_clean %>% count(word, sort=TRUE)
wordcloud2(year_words, size = 1.6, color = (c("green","blue")))

```

The past-year month list resembles the past-year list.

```{r}

month_words <- text_month_clean %>% count(word, sort=TRUE)
wordcloud2(year_words, size = 1.6, color = (c("purple","blue")))

```

## Finding 4: Comparing word frequency to another science source

The words in r/science top posts titles are very weakly correlated with the words from the homepages of a popular science source, [Frontiers](https://blog.frontiersin.org/). I chose Frontiers because  it is an open-source journal with a webpage that posts article summaries and science news, much like r/science does. There are some similarities between this wordcloud and the all-time wordcloud, but some of the more frequently-used words in the Reddit wordcloud are missing from the Frontiers wordcloud (e.g. "sex", "life", and "U.S.". Furthermore, no one word appears as often as the top words in Reddit titles: "DNA" appears 6 times, and "science" appears 4 times.

```{r warning=FALSE, include=FALSE}
frontiers_clean <- read_csv("~/Documents/Projects/reddit-science-analysis/frontiers_clean.csv", show_col_types = FALSE)

```
```{r}
# word cloud
frontiers_words <- frontiers_clean %>% count(word, sort=TRUE)
wordcloud2(frontiers_words, size = 1.6, color=(c("red","purple")))
```

## Finding 5: Sentiment analysis of titles

I used the NRC Word-Emotion Association Lexicon (C) to conduct sentiment analysis on post titles. Posts contain more positive words than negative words. Other common emotions in post titles are "trust", "anticipation", and "fear". Sentiment analysis showed similar results for all 3 time periods.

```{r}
# sentiment analysis

get_sentiments("nrc")

all_time_sentiment <- text_all_clean %>% 
  inner_join(get_sentiments("nrc"), by = "word")

year_sentiment <- text_year_clean %>% 
  inner_join(get_sentiments("nrc"), by = "word")

month_sentiment <- text_month_clean %>% 
  inner_join(get_sentiments("nrc"), by = "word")

sentiment_all_plot <- all_time_sentiment %>% 
  group_by(sentiment) %>% 
  summarize(num_words = n()) %>% 
  arrange(desc(num_words)) %>% 
  ggplot(aes(x = num_words, y=reorder(sentiment, num_words), fill=sentiment)) +
  geom_col(show.legend=FALSE) +
  gghighlight(num_words > 100) +
  labs(
    title = "Sentiment analysis of post titles",
    subtitle = "Top 100 posts of all time",
    x = "Number of words",
    y = "Sentiment"
  ) +
  theme_minimal()
sentiment_all_plot

sentiment_year_plot <- year_sentiment %>% 
  group_by(sentiment) %>% 
  summarize(num_words = n()) %>% 
  arrange(desc(num_words)) %>% 
  ggplot(aes(x = num_words, y=reorder(sentiment, num_words), fill=sentiment)) +
  geom_col(show.legend=FALSE) +
  gghighlight(num_words > 100) +
  labs(
    title = "Sentiment analysis of post titles",
    subtitle = "Top 100 posts last year (2022)",
    x = "Number of words",
    y = "Sentiment"
  ) +
  theme_minimal()
sentiment_year_plot

sentiment_month_plot <- month_sentiment %>% 
  group_by(sentiment) %>% 
  summarize(num_words = n()) %>% 
  arrange(desc(num_words)) %>% 
  ggplot(aes(x = num_words, y=reorder(sentiment, num_words), fill=sentiment)) +
  geom_col(show.legend=FALSE) +
  gghighlight(num_words > 100) +
  labs(
    title = "Sentiment analysis of post titles",
    subtitle = "Top 100 posts last month (December 2022)",
    x = "Number of words",
    y = "Sentiment"
  ) +
  theme_minimal()
sentiment_month_plot

```


# Conclusions

Overall, r/science most popular posts are pop-science, and favor psychology, social science, and health topics. Posts on r/science come from many sources, though within the past year psypost.org has become overrepresented on the site, and this trend continued through December 2022. These posts are highly upvoted and have high engagement. 

The overall sentiment of posts on r/science is slightly positive. Some words appear very frequently in the post titles. The most frequent post title words are "everyday" words, not specific to science. By contrast, the title words of an open-source journal's blog include more scientific terms.



## Citations

NRC Word-Emotion Association Lexicon: Copyright (C) 2011 National Research Council Canada (NRC)

Version: 0.92
Publicly Released: 10 July 2011
Created By: Dr. Saif M. Mohammad, Dr. Peter Turney
Home Page: http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm

Readme Last Updated: August 2022
Automatic translations from English to 108 languages was last updated: August 2022

Contact: Dr. Saif M. Mohammad (Senior Research Scientist, National Research Council Canada)
saif.mohammad@nrc-cnrc.gc.ca, uvgotsaif@gmail.com
